\label{sec:introduction}

Deep learning is a subset of machine learning that uses artificial neural networks to learn from unstructured data. Deep learning has become very popular in recent years due to its performance in a number of fields, such as image recognition, speech recognition, machine translation and more. Artificial neural networks are mathematical models inspired by the workings of the human brain. They are composed of several layers of interconnected neurons. Each neuron has an activation function that calculates the neuron's output according to its inputs. There are several types of artificial neural network architecture. These architectures have been developed to address specific problems, such as convolution architecture for image recognition, recurrent neural networks for speech recognition, and so on. In this paper, we focus on convolution architecture, as it can handle a wide range of problems, such as image recognition, text recognition and sequence recognition. Moreover, convolution architecture is part of feed-forward architectures, a class of artificial neural networks composed of several layers of neurons and containing no cycles. Feed-forward neural networks are the most widely used in deep learning.\\


\subsection{Convolutional Neural Network}
In this paper, we will refer to deep learning programs as those containing convolutional neural networks (CNN). Convolutional Neural Network consists of the following three main stages:\\

\subsubsection*{Convolution}
Convolution is the action of passing filters over an image to extract patterns. A filter is a classical neural network that scans an image part by part to detect a pattern. The filter scans a window (part) of the image and then calculates the output in one pixel. The filter then browses the entire image to calculate the output of each pixel in a new image. The new image is called a feature map. Convolution increases the depth of the input image by generating multiple feature maps while reducing its size. Recall that each depth layer is an image (reduced in size) generated by a filter.\\

\subsubsection*{Pooling}
Pooling is an operation that reduces image size by selecting the largest pixel in a window. This reduces computation and memory consumption. It also reduces Overfitting on training data, as information on the position of detected patterns is lost. So we're going to use pooling on the feature maps generated by convolution. Convolution is followed by pooling, and this operation is repeated several times. The deeper we go in the convolution, the more precise patterns are detected, likely to facilitate classification.\\

\subsubsection*{Flatten}
The flatten operation transforms the feature maps generated by convolution and pooling into a single vector. This vector is then used as the input to a classical neural network (\emph{dense layer}). The output of the convolutional neural network will come from this neuron.

\subsection{Design smell}
\label{sec:codeSmell}
A code smell is a poor design and implementation choice that can have a negative impact on software quality \cite{fowler1997refactoring}. Just like any other program, deep learning programs can contain code smells. In this paper, we focus on design smells because they are introduced early in the software development cycle and can have a significant negative impact on software performance and quality. These smells are introduced by the developer during the software design phase.\\ Nikanjam et al. proposed a list of 8 design smells for feed-forward learning programs \cite{nikanjam2019deep}. They found these smells in literature reviews and the open-source platforms Github and StackOverflow. Their empirical study showed that the proposed smells are perceived as relevant by developers. We have therefore used this list of smells to propose an automatic smell detection system for developers.\\ The 8 proposed smells are classified into two main categories as follows:
\begin{enumerate}
    \item \textbf{Design smells during convolution and pooling}: These smells are
          introduced when feature maps are formed. They are linked to the size of the
          feature map or filter and the layer layout. The list of smells is presented in
          the table \ref{tab:convolutionalSmellsConv} below.
    \item \textbf{Design smells linked to the use of regularization methods}: These
          smells are introduced when using regularization. They are linked to the use of
          regularization methods such as the dropout layer and the layout of these
          methods. The list of smells is presented in the table
          \ref{tab:convolutionalSmellsReg} below.
\end{enumerate}


\begin{table*}[h]
    \centering
    \caption{\emph{Design smells in convolution and pooling layers}}
    \label{tab:convolutionalSmellsConv}
    \begin{tabular}{ll}
        \toprule
        \multicolumn{1}{c}{\textbf{Design Smell}} &
        \multicolumn{1}{c}{\textbf{Description}}                                                                                                                 \\ \midrule
        Non-expanding feature map                 &
        \begin{tabular}[c]{@{}l@{}}Keep the same number of features or reduce it as the architecture becomes deeper.\end{tabular}                                \\
        Losing local correlation                  &
        \begin{tabular}[c]{@{}l@{}}Start with a relatively large window size for spatial filtering and maintain it for all convolutional layers.\end{tabular}    \\
        Heterogeneous blocks of CNNs              &
        \begin{tabular}[c]{@{}l@{}}Build a deeper model by stacking only a set of convolution and pooling layers without appropriate configuration.\end{tabular} \\
        Too much down-sampling                    &
        \begin{tabular}[c]{@{}l@{}}Pooling right after each convolutional layer, especially for the first layers.\end{tabular}                                   \\
        Non-dominating down-sampling              &
        Use of average-pooling.                                                                                                                                  \\ \bottomrule
    \end{tabular}
\end{table*}


\begin{table*}[h]
    \centering
    \caption{\emph{Design smells related to regularization}}
    \label{tab:convolutionalSmellsReg}
    \begin{tabular}{ll}
        \toprule
        \multicolumn{1}{c}{\textbf{Design Smell}} & \multicolumn{1}{c}{\textbf{Description}}
        \\ \midrule
        Useless Dropout                           & Using Dropout before pooling layers.                                                                                \\
        Bias with Batchnorm                       &
        \begin{tabular}[c]{@{}l@{}}Keep the bias values in the layers when using batchnorm. FNN learning layers are biased with different initializations.\end{tabular} \\
        Non-representative Statistics Estimation  & Use of batchnorm after dropout.                                                                                     \\ \bottomrule
    \end{tabular}
\end{table*}



\subsection{Modeling}

Thomas Kühne defines a model as an artifact formulated in a modeling language, such as UML, that describes a system using different types of diagrams \cite{kuhne2006matters}. This abstract representation of a system is defined by another model called a metamodel. A model allows a system to be analyzed, understood and transformed efficiently. Its operations can be automated \cite{gonzalez2014formal}. There are several types of models, such as behavioral models (representing the dynamic behavior of the system) or design models (representing the static structure of the system).\\In this paper, we are interested in design models because the smells studied are represented in static form and are introduced at the system structure level.\\

We'll also be using \textbf{\emph{FAST}} (\emph{Famix Abstract Syntax Tree}) models to represent the systems studied in this paper. The \emph{FAST} models are syntax trees derived from the Pharo object-oriented programming language \cite{black2010pharo, bergel2013deep, zaitsev2020characterizing}. It inherits the \textbf{\emph{Famix}} model, which is defined by Tichelaar et al. as a language-independent representation of object-oriented source code. It is an entity-relationship model that models object-oriented source code at the program entity level \cite{891485} \cite{demeyer1999famix}. The \emph{FAST} models are developed in the Moose retroengineering environment \cite{ducasse2000moose}.\\\\



\subsection{Motivation and research questions}
In Nikanjam et al.'s study, developers identified not only the proposed design smells, but also their relevance and hence the value of avoiding them. However, there is no automatic smell detection system in the literature. In this paper, we propose such a system, which will enable smell detection in CNN deep learning programs. Moreover, our system will cover the most widely used open source Frameworks on the market. To this aim, we have defined the following research questions:\\

\emph{\\RQ1:\RQOne} % La technique de détection

\emph{\\RQ2:\RQTwo} % La répartition

\emph{\\RQ3:\RQThree} % Les relations

Our paper is organized as follows: section \ref{sec:background} presents past work related to our topic. Section \ref{sec:studyDesign} describes the data collection and processing. Section \ref{sec:results} presents the results. Section \ref{sec:discussion} discusses the results presented above. Section \ref{sec:threats} presents threats to validity and section \ref{sec:limitationsFutureWork} presents the limitations of our study and future work.
